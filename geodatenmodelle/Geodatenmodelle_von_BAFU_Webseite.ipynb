{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Geodatenmodelle von BAFU Webseite extrahieren\n",
        "Das BAFU publiziert auf folgender Webseite minimale Geodatenmodelle mit Dokumentation in PDF-Files.\n",
        "https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle.html\n",
        "\n",
        "Mit folgendem Skript werden die Metadaten (URL, Beschreibung, Titel, Datum) der publizierten ZIP-Files extrahiert. Die Metadaten werden anschliessend in den BAFU Datenkatalog integriert."
      ],
      "metadata": {
        "id": "TYoTMW-oHOff"
      },
      "id": "TYoTMW-oHOff"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import datetime"
      ],
      "metadata": {
        "id": "_tUPlTU4W4Uu"
      },
      "id": "_tUPlTU4W4Uu",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alle BAFU-Webseiten mit MGDM ZIP-Files\n",
        "BafuSeiten = {\"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle.html\":\"BAFU\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/abfall--geodatenmodelle.html\": \"Abfall\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/altlasten--geodatenmodelle.html\": \"Altlasten\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/biodiversitaet--geodatenmodelle.html\": \"Biodiversität\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/biotechnologie--geodatenmodelle.html\": \"Biotechnologie\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/boden--geodatenmodelle.html\": \"Boden\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/chemikalien--geodatenmodelle.html\": \"Chemikalien\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/landschaft--geodatenmodelle.html\": \"Landschaft\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/laerm--geodatenmodelle.html\": \"Lärm\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/luft--geodatenmodelle.html\": \"Luft\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/naturgefahren--geodatenmodelle.html\": \"Naturgefahren\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/stoerfallvorsorge--geodatenmodelle.html\": \"Störfallvorsorge\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/wald--geodatenmodelle.html\": \"Wald\",\n",
        "              \"https://www.bafu.admin.ch/bafu/de/home/zustand/daten/geodatenmodelle/wasser--geodatenmodelle.html\": \"Wasser\"}"
      ],
      "metadata": {
        "id": "Ew52eaIYX4wZ"
      },
      "id": "Ew52eaIYX4wZ",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion mit welcher das Datum aus einem String extrahiert wird\n",
        "def extract_date_from_text(text):\n",
        "    # Look for date patterns in the text (e.g., DD.MM.YYYY, YYYY-MM-DD)\n",
        "    date_patterns = [\n",
        "        r'\\d{2}\\.\\d{2}\\.\\d{4}',  # DD.MM.YYYY\n",
        "        r'\\d{4}-\\d{2}-\\d{2}',  # YYYY-MM-DD\n",
        "        r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', # D.M.YYYY or DD.M.YYYY or D.MM.YYYY\n",
        "        r'\\d{1,2}\\.\\d{1,2}\\.\\d{2}' # D.M.YY etc\n",
        "    ]\n",
        "    for pattern in date_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            try:\n",
        "                # Attempt to parse the matched date\n",
        "                # Add more date formats as needed\n",
        "                if '.' in match.group(0):\n",
        "                    date_object = datetime.datetime.strptime(match.group(0), '%d.%m.%Y')\n",
        "                elif '-' in match.group(0):\n",
        "                    date_object = datetime.datetime.strptime(match.group(0), '%Y-%m-%d')\n",
        "                return date_object.strftime('%Y-%m-%d') # Standardize to YYYY-MM-DD\n",
        "            except ValueError:\n",
        "                # If parsing fails, it might not be a valid date despite matching pattern\n",
        "                continue\n",
        "    return None # Return None if no date is found or parsed"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ga6H1OGDU4PW"
      },
      "id": "Ga6H1OGDU4PW",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion welche Metadaten aller MGDM ZIP-Files von der BAFU Webseite holt\n",
        "\n",
        "def extract_zip_metadata(url, keyword):\n",
        "    \"\"\"\n",
        "    Extracts metadata of ZIP files linked on a given URL using BeautifulSoup.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to scrape.\n",
        "        keyword (str): A keyword associated with the URL (used in the output DataFrame).\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing metadata (url, filename, description,\n",
        "                          link_text, extracted_date, keyword) for each ZIP file found.\n",
        "                          Returns an empty DataFrame if no ZIP files are found or\n",
        "                          if an error occurs during the request.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame on error\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all anchor tags (links)\n",
        "    links = soup.find_all('a', href=True)\n",
        "\n",
        "    zip_data = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link['href']\n",
        "        # Check if the link points to a ZIP file (case-insensitive)\n",
        "        if href.lower().endswith('.zip'):\n",
        "            # Get the full URL if it's a relative path\n",
        "            if href.startswith('/'):\n",
        "                zip_url = f\"https://www.bafu.admin.ch{href}\"\n",
        "            elif href.startswith('./'):\n",
        "                # This relative path handling might need adjustment based on the specific URL structure\n",
        "                # Assuming the base for './' is the directory of the current page\n",
        "                base_url_match = re.match(r'(https?://[^/]+/.*/)', url)\n",
        "                if base_url_match:\n",
        "                    base_url = base_url_match.group(1)\n",
        "                    zip_url = f\"{base_url}{href[2:]}\"\n",
        "                else:\n",
        "                     zip_url = href # Fallback to original href if base cannot be determined\n",
        "            else:\n",
        "                zip_url = href\n",
        "\n",
        "            # Attempt to get metadata (e.g., filename, potential description from surrounding text)\n",
        "            filename_match = re.search(r'/([^/]+\\.zip)$', zip_url)\n",
        "            filename = filename_match.group(1) if filename_match else 'N/A'\n",
        "\n",
        "            # Try to find a descriptive text near the link\n",
        "            description = 'N/A'\n",
        "            # Look for preceding text or sibling elements\n",
        "            prev_sibling = link.previous_sibling\n",
        "            if prev_sibling and isinstance(prev_sibling, str):\n",
        "                description = prev_sibling.strip()\n",
        "            elif link.parent:\n",
        "                # Check if the parent has relevant text\n",
        "                parent_text = link.parent.get_text().replace(link.get_text(), '').strip()\n",
        "                if parent_text:\n",
        "                     description = parent_text\n",
        "                else:\n",
        "                    # Look for text within the immediate siblings\n",
        "                    for sibling in link.find_previous_siblings():\n",
        "                        if isinstance(sibling, str) and sibling.strip():\n",
        "                            description = sibling.strip()\n",
        "                            break\n",
        "                    if description == 'N/A':\n",
        "                        for sibling in link.find_next_siblings():\n",
        "                             if isinstance(sibling, str) and sibling.strip():\n",
        "                                description = sibling.strip()\n",
        "                                break\n",
        "\n",
        "            # Extract the text within the <a> tag\n",
        "            link_text = link.get_text().strip()\n",
        "\n",
        "            # Extract date from link_text\n",
        "            extracted_date = extract_date_from_text(link_text)\n",
        "\n",
        "\n",
        "            zip_data.append({'url': zip_url,\n",
        "                             'filename': filename,\n",
        "                             'description': description,\n",
        "                             'link_text': link_text,\n",
        "                             'extracted_date': extracted_date,\n",
        "                             'keyword': keyword})\n",
        "\n",
        "    # Create a pandas DataFrame\n",
        "    df = pd.DataFrame(zip_data)\n",
        "\n",
        "    # Clean link_text by removing content in parentheses\n",
        "    df['link_text_cleaned'] = df['link_text'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x).strip())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ZPg8kbJLZt_i"
      },
      "id": "ZPg8kbJLZt_i",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alle Webseiten aufrufen und alle Metadaten als CSV speichern\n",
        "\n",
        "# Create an empty list to store the results from each page\n",
        "all_zip_metadata = []\n",
        "\n",
        "# Iterate over the BafuSeiten dictionary\n",
        "for url, keyword in BafuSeiten.items():\n",
        "    # Call the function for each entry\n",
        "    df_page = extract_zip_metadata(url, keyword)\n",
        "    # Append the resulting DataFrame to the list\n",
        "    if not df_page.empty:\n",
        "        all_zip_metadata.append(df_page)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "if all_zip_metadata:\n",
        "    final_df = pd.concat(all_zip_metadata, ignore_index=True)\n",
        "    print(\"\\nCombined DataFrame of ZIP metadata:\")\n",
        "    print(f\"\\nTotal number of ZIP files found: {len(final_df)}\")\n",
        "else:\n",
        "    print(\"\\nNo ZIP files found on any of the specified BAFU pages.\")\n",
        "    final_df = pd.DataFrame() # Create an empty DataFrame if no data was collected\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhatdZlTbWou",
        "outputId": "ff993ce4-27b5-4bcb-fa1a-01b8e80dea39"
      },
      "id": "FhatdZlTbWou",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combined DataFrame of ZIP metadata:\n",
            "\n",
            "Total number of ZIP files found: 82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAw6wsAKe-4x"
      },
      "id": "kAw6wsAKe-4x",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: in final_df[\"description\"] die Zeilenumbrüche löschen und durch \", \" ersetzen\n",
        "\n",
        "final_df[\"description\"] = final_df[\"description\"].str.replace('\\n', ', ', regex=False)\n",
        "final_df[\"title\"] = \"Geodatenmodell \" + final_df[\"link_text_cleaned\"]\n",
        "final_df[\"kontakt\"] = \"gis@bafu.admin.ch\"\n",
        "final_df[\"Typ\"] = \"Geodatenmodell\"\n",
        "\n",
        "Geodatenmodelle_df = final_df.rename(columns={'keyword': 'keyword',\n",
        "                                              'title': 'title',\n",
        "                                              'description': 'description',\n",
        "                                              'extracted_date':'modified',\n",
        "                                              'Typ': 'Typ',\n",
        "                                              'kontakt': 'Kontakt',\n",
        "                                              'url': 'URL'\n",
        "                                              })\n",
        "Geodatenmodelle_df = Geodatenmodelle_df[['keyword', 'title', 'description', 'modified', 'Typ', 'Kontakt', 'URL']]"
      ],
      "metadata": {
        "id": "MEUeQlv7hHsi"
      },
      "id": "MEUeQlv7hHsi",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Geodatenmodelle_df exportieren als csv mit \";\" als Trennzeichen und\n",
        "\n",
        "Geodatenmodelle_df.to_csv('Geodatenmodelle.csv', sep=';', index=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9SE4llDEhzfl"
      },
      "id": "9SE4llDEhzfl",
      "execution_count": 65,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}